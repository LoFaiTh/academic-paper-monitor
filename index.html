
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Academic Paper Monitor</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }
        .header { background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 20px; }
        .field-section { margin-bottom: 30px; }
        .paper { border: 1px solid #ddd; padding: 15px; margin-bottom: 15px; border-radius: 5px; }
        .paper-title { font-weight: bold; color: #0366d6; margin-bottom: 5px; }
        .paper-meta { color: #666; font-size: 14px; margin-bottom: 10px; }
        .paper-summary { line-height: 1.5; }
        .field-title { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
        .last-updated { text-align: right; color: #666; font-size: 12px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Academic Paper Monitor</h1>
        <p>Monitoring new papers in: Structure from Motion (SFM), SLAM, 3D Gaussian Splatting (3DGS), and Neural Radiance Fields (NeRF)</p>
        <p><strong>Total new papers found:</strong> 348</p>
        <p class="last-updated">Last updated: 2025-07-05 09:19:01 UTC</p>
    </div>
<div class="field-section"><h2 class="field-title">SFM (87 papers)</h2>
                        <div class="paper">
                            <div class="paper-title">MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Renhao Wang, Haoran Geng, Tingle Li and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.RO, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02864v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robots must integrate multiple sensory modalities to act effectively in the
real world. Yet, learning such multimodal policies at scale remains
challenging. Simulation offers a viable solution, but while vision has
benefited from high-fidelity simulators, other modalities (e.g. sound) can be
notoriously difficult to simulate. As a result, sim-to-real transfer has
succeeded primarily in vision-based tasks, with multimodal transfer still
largely unrealized. In this work, we tackle these challenges...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuqi Wu, Wenzhao Zheng, Jie Zhou and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02863v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Dense 3D scene reconstruction from an ordered sequence or unordered image
collections is a critical step when bringing research in computer vision into
practical scenarios. Following the paradigm introduced by DUSt3R, which unifies
an image pair densely into a shared coordinate system, subsequent methods
maintain an implicit memory to achieve dense 3D reconstruction from more
images. However, such implicit memory is limited in capacity and may suffer
from information loss of earlier frames. We p...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RefTok: Reference-Based Tokenization for Video Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiang Fan, Xiaohang Sun, Kushan Thakkar and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02862v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Effectively handling temporal redundancy remains a key challenge in learning
video models. Prevailing approaches often treat each set of frames
independently, failing to effectively capture the temporal dependencies and
redundancies inherent in videos. To address this limitation, we introduce
RefTok, a novel reference-based tokenization method capable of capturing
complex temporal dynamics and contextual information. Our method encodes and
decodes sets of frames conditioned on an unquantized ref...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiaer Xia, Bingkui Tong, Yuhang Zang and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02859v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AnyI2V: Animating Any Conditional Image with Motion Control</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziye Li, Hao Luo, Xincheng Shuai and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02857v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in video generation, particularly in diffusion models,
have driven notable progress in text-to-video (T2V) and image-to-video (I2V)
synthesis. However, challenges remain in effectively integrating dynamic motion
signals and flexible spatial constraints. Existing T2V methods typically rely
on text prompts, which inherently lack precise control over the spatial layout
of generated content. In contrast, I2V methods are limited by their dependence
on real images, which restricts ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziqi Miao, Yi Ding, Lijun Li and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02844v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the vi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ying Yu, Hang Xiao, Siyao Li and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02827v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensiv...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Panpan Ji, Junni Song, Hang Xiao and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02826v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extrac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Liheng Zhang, Lexi Pang, Hang Ye and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02792v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition i...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangfeng Wang, Xiao Li, Yadong Wei and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02790v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Danrong Zhang, Huili Huang, N. Simrill Smith and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.SI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02781v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage sever...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Grounding Intelligence in Movement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Melanie Segado, Felipe Parodi, Jordan K. Matelsky and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.AI, cs.CV, cs.LG, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02771v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alex Colagrande, Paul Caillon, Eva Feillet and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02748v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the fi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Prompt learning with bounding box constraints for medical image segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Mélanie Gaillochet, Mehrdad Noori, Sahar Dastani and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02743v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Pixel-wise annotations are notoriously labourious and costly to obtain in the
medical domain. To mitigate this burden, weakly supervised approaches based on
bounding box annotations-much easier to acquire-offer a practical alternative.
Vision foundation models have recently shown noteworthy segmentation
performance when provided with prompts such as points or bounding boxes. Prompt
learning exploits these models by adapting them to downstream tasks and
automating segmentation, thereby reducing u...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuxuan Wang, Tianwei Cao, Huayu Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02714v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image generation has achieved remarkable progress with the development of
large-scale text-to-image models, especially diffusion-based models. However,
generating human images with plausible details, such as faces or hands, remains
challenging due to insufficient supervision of local regions during training.
To address this issue, we propose FairHuman, a multi-objective fine-tuning
approach designed to enhance both global and local generation quality fairly.
Specifically, we first construct thre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qin Guo, Ailing Zeng, Dongxu Yue and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02713v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Although significant advancements have been achieved in the progress of
keypoint-guided Text-to-Image diffusion models, existing mainstream
keypoint-guided models encounter challenges in controlling the generation of
more general non-rigid objects beyond humans (e.g., animals). Moreover, it is
difficult to generate multiple overlapping humans and animals based on keypoint
controls solely. These challenges arise from two main aspects: the inherent
limitations of existing controllable methods and ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qi Xu, Dongxu Wei, Lingzhe Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02705v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruct...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangyang Luo, Ye Zhu, Yunfei Liu and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02691v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appea...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qingyu Fan, Yinghao Cai, Chao Li and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.RO, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02672v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robotic grasping faces challenges in adapting to objects with varying shapes
and sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method
that integrates multi-scale feature extraction with contrastive feature
enhancement for self-adaptive grasping. We propose a query-based interaction
between high-level and low-level features through the Insight Transformer,
while the Empower Transformer selectively attends to the highest-level
features, which synergistically strikes a balance...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Francesco Di Salvo, Hanh Huyen My Nguyen, Christian Ledig<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.LG, cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02671v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, red...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziyin Zhou, Yunpeng Luo, Yuanchen Wu and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02664v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Riccardo Gallon, Fabian Schiemenz, Alessandra Menicucci and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02602v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The increasing importance of Vision-Based Navigation (VBN) algorithms in
space missions raises numerous challenges in ensuring their reliability and
operational robustness. Sensor faults can lead to inaccurate outputs from
navigation algorithms or even complete data processing faults, potentially
compromising mission objectives. Artificial Intelligence (AI) offers a powerful
solution for detecting such faults, overcoming many of the limitations
associated with traditional fault detection methods...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tan Pan, Zhaorui Tan, Kaiyu Guo and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02581v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Parametric shape models for vessels learned from segmentations via differentiable voxelization</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alina F. Dima, Suprosanna Shit, Huaqi Qiu and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02576v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Vessels are complex structures in the body that have been studied extensively
in multiple representations. While voxelization is the most common of them,
meshes and parametric models are critical in various applications due to their
desirable properties. However, these representations are typically extracted
through segmentations and used disjointly from each other. We propose a
framework that joins the three representations under differentiable
transformations. By leveraging differentiable voxe...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Buzhen Huang, Chen Li, Chongyang Xu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02565v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurat...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ruicheng Wang, Sicheng Xu, Yue Dong and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02546v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover th...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Initialization-free Calibrated Bundle Adjustment</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Carl Olsson, Amanda Nilsson<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23808v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method th...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziao Liu, Zhenjia Li, Yifeng Shi and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23611v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ying Yu, Hang Xiao, Siyao Li and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02827v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensiv...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Panpan Ji, Junni Song, Hang Xiao and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02826v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extrac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Danrong Zhang, Huili Huang, N. Simrill Smith and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.SI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02781v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage sever...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">APT: Adaptive Personalized Training for Diffusion Models with Limited Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JungWoo Chae, Jiyoon Kim, JaeWoong Choi and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, 60J60, 68T07, I.2.6; I.2.10; I.4.9<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02687v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's in...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Fair Deepfake Detectors Can Generalize</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Harry Cheng, Ming-Hui Liu, Yangyang Guo and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02645v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distrib...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hazal Mogultay Ozcan, Sinan Kalkan, Fatos T. Yarman-Vural<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02619v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Takuro Kawada, Shunsuke Kitada, Sota Nemoto and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02212v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Controllable Real Image Denoising with Camera Parameters</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Youngjin Oh, Junhyeong Kwon, Keuntek Lee and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01587v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these se...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AVC-DPO: Aligned Video Captioning via Direct Preference Optimization</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiyang Tang, Hengyi Li, Yifan Du and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01492v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specif...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Runze Cheng, Xihang Qiu, Ming Li and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01254v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> An Le, Nehal Mehta, William Freeman and 9 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> eess.IV, cs.CV, eess.SP<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00743v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this study, we developed deep learning-based method to classify the type
of surgery performed for epiretinal membrane (ERM) removal, either internal
limiting membrane (ILM) removal or ERM-alone removal. Our model, based on the
ResNet18 convolutional neural network (CNN) architecture, utilizes
postoperative optical coherence tomography (OCT) center scans as inputs. We
evaluated the model using both original scans and scans preprocessed with
energy crop and wavelet denoising, achieving 72% accu...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Rectifying Magnitude Neglect in Linear Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qihang Fan, Huaibo Huang, Yuang Ai and 1 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00698v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the un...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xingjun Wang, Lianlei Shan<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with lim...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Navigating with Annealing Guidance Scale in Diffusion Space</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Shai Yehezkel, Omer Dahary, Andrey Voynov and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.GR, cs.AI, cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.24108v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Initialization-free Calibrated Bundle Adjustment</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Carl Olsson, Amanda Nilsson<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23808v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method th...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Single Image Test-Time Adaptation via Multi-View Co-Training</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Smriti Joshi, Richard Osuala, Lidia Garrucho and 4 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23705v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging d...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Pyramidal Patchification Flow for Visual Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hui Li, Baoyou Chen, Liwei Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23543v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is ac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuhao Huang, Yueyue Xu, Haoran Dou and 4 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23538v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Sample Margin-Aware Recalibration of Temperature Scaling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Haolan Guo, Linwei Tao, Haoyang Luo and 2 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.LG, cs.AI, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23492v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer fro...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhaojie Zeng, Yuesong Wang, Chao Yang and 2 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23479v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Huang, Long Bai, Beilei Cui and 7 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Aradhana Mishra, Bumshik Lee<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.MM, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23254v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CoreMark: Toward Robust and Universal Text Watermarking Technique</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiale Meng, Yiming Li, Zheming Lu and 3 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV, cs.CR, cs.MM<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23066v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking fr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Senkang Hu, Yihang Tao, Guowen Xu and 5 others<br>
                                <strong>Published:</strong> 2025-06-28<br>
                                <strong>Categories:</strong> cs.CV, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.22890v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adap...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xin Zhou, Dingkang Liang, Kaijin Chen and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02860v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiaer Xia, Bingkui Tong, Yuhang Zang and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02859v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziqi Miao, Yi Ding, Lijun Li and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02844v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the vi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">No time to train! Training-Free Reference-Based Instance Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Miguel Espinosa, Chenhongyi Yang, Linus Ericsson and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02798v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The performance of image segmentation models has historically been
constrained by the high cost of collecting large-scale annotated data. The
Segment Anything Model (SAM) alleviates this original problem through a
promptable, semantics-agnostic, segmentation paradigm and yet still requires
manual visual-prompts or complex domain-dependent prompt-generation rules to
process a new image. Towards reducing this new burden, our work investigates
the task of object segmentation when provided with, alt...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Liheng Zhang, Lexi Pang, Hang Ye and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02792v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition i...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangfeng Wang, Xiao Li, Yadong Wei and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02790v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangyang Luo, Ye Zhu, Yunfei Liu and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02691v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appea...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziyin Zhou, Yunpeng Luo, Yuanchen Wu and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02664v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Weili Xu, Enxin Song, Wenhao Chai and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02591v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual tok...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Buzhen Huang, Chen Li, Chongyang Xu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02565v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurat...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Luca Parolari, Andrea Cherubini, Lamberto Ballan and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02493v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Automated polyp counting in colonoscopy is a crucial step toward automated
procedure reporting and quality control, aiming to enhance the
cost-effectiveness of colonoscopy screening. Counting polyps in a procedure
involves detecting and tracking polyps, and then clustering tracklets that
belong to the same polyp entity. Existing methods for polyp counting rely on
self-supervised learning and primarily leverage visual appearance, neglecting
temporal relationships in both tracklet feature learning...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hailong Yan, Junjian Huang, Tingwen Huang<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02445v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Current methods for restoring underexposed images typically rely on
supervised learning with paired underexposed and well-illuminated images.
However, collecting such datasets is often impractical in real-world scenarios.
Moreover, these methods can lead to over-enhancement, distorting
well-illuminated regions. To address these issues, we propose IGDNet, a
Zero-Shot enhancement method that operates solely on a single test image,
without requiring guiding priors or training data. IGDNet exhibits ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Holistic Tokenizer for Autoregressive Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Anlin Zheng, Haochen Wang, Yucheng Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02358v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rafic Nader, Vincent L'Allinec, Romain Bourcier and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02349v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zecheng Zhao, Selena Song, Tong Chen and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02316v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation
metrics primarily capture visual quality and temporal consistency, offering
limited insight into how synthetic videos perform in downstream tasks such as
text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset
and benchmark designed to evaluate the utility of synthetic videos for building
retrieval models. Based on 800 diverse user queries derived from MSRVTT
training split, we generate synthetic v...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LMPNet for Weakly-supervised Keypoint Discovery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Pei Guo, Ryan Farrell<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hanbo Bi, Yulong Xu, Ya Li and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02294v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on n...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wangbin Ding, Lei Li, Junyi Qiu and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02289v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Myocardial infarction (MI) is a leading cause of death worldwide. Late
gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)
imaging can respectively identify scarring and edema areas, both of which are
essential for MI risk stratification and prognosis assessment. Although
combining complementary information from multi-sequence CMR is useful,
acquiring these sequences can be time-consuming and prohibitive, e.g., due to
the administration of contrast agents. Cine CMR is a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> De Cheng, Zhipeng Xu, Xinyang Jiang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02288v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Juntao Liu, Liqiang Niu, Wenchao Chen and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02279v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens thr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Feizhen Huang, Yu Wu, Yutian Lin and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.MM<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02271v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fanghai Yi, Zehong Zheng, Zexiao Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02270v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) fo...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zeyu Lei, Hongyuan Yu, Jinlin Wu and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02252v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamic...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Understanding Trade offs When Conditioning Synthetic Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Brandon Trabucco, Qasim Wani, Benjamin Pikus and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02217v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulati...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Takuro Kawada, Shunsuke Kitada, Sota Nemoto and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02212v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising a...</div>
                        </div>
                        </div><div class="field-section"><h2 class="field-title">SLAM (68 papers)</h2>
                        <div class="paper">
                            <div class="paper-title">RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> David Hunt, Shaocheng Luo, Spencer Hallyburton and 4 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.RO, cs.AR, cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00937v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhen Tan, Xieyuanli Chen, Lei Feng and 4 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23207v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ying Yu, Hang Xiao, Siyao Li and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02827v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensiv...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Panpan Ji, Junni Song, Hang Xiao and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02826v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extrac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Liheng Zhang, Lexi Pang, Hang Ye and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02792v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition i...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DexVLG: Dexterous Vision-Language-Grasp Model at Scale</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiawei He, Danshi Li, Xinqiang Yu and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02747v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">As large models gain traction, vision-language-action (VLA) systems are
enabling robots to tackle increasingly complex tasks. However, limited by the
difficulty of data collection, progress has mainly focused on controlling
simple gripper end-effectors. There is little research on functional grasping
with large models for human-like dexterous hands. In this paper, we introduce
DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction
aligned with language instructions using...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuxuan Wang, Tianwei Cao, Huayu Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02714v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image generation has achieved remarkable progress with the development of
large-scale text-to-image models, especially diffusion-based models. However,
generating human images with plausible details, such as faces or hands, remains
challenging due to insufficient supervision of local regions during training.
To address this issue, we propose FairHuman, a multi-objective fine-tuning
approach designed to enhance both global and local generation quality fairly.
Specifically, we first construct thre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qi Xu, Dongxu Wei, Lingzhe Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02705v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruct...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">APT: Adaptive Personalized Training for Diffusion Models with Limited Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JungWoo Chae, Jiyoon Kim, JaeWoong Choi and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, 60J60, 68T07, I.2.6; I.2.10; I.4.9<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02687v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's in...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhe Yee Tan<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02668v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Colorectal polyp segmentation is critical for early detection of colorectal
cancer, yet weak and low contrast boundaries significantly limit automated
accuracy. Existing deep models either blur fine edge details or rely on
handcrafted filters that perform poorly under variable imaging conditions. We
propose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects
directional, parameter free Haar wavelet edge maps into each decoder stage to
recalibrate semantic features. Our two mai...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ruicheng Wang, Sicheng Xu, Yue Dong and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02546v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover th...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Abiam Remache González, Meriem Chagour, Timon Bijan Rüth and 12 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, I.2.10; I.4.8<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02519v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">This paper introduces IMASHRIMP, an adapted system for the automated
morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing
genetic selection tasks in aquaculture. Existing deep learning and computer
vision techniques were modified to address the specific challenges of shrimp
morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination
modules, based on a modified ResNet-50 architecture, to classify images by the
point of view and determine rostrum inte...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Automatic Labelling for Low-Light Pedestrian Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Dimitrios Bouzoulas, Eerik Alamikkotervo, Risto Ojala<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02513v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Pedestrian detection in RGB images is a key task in pedestrian safety, as the
most common sensor in autonomous vehicles and advanced driver assistance
systems is the RGB camera. A challenge in RGB pedestrian detection, that does
not appear to have large public datasets, is low-light conditions. As a
solution, in this research, we propose an automated infrared-RGB labeling
pipeline. The proposed pipeline consists of 1) Infrared detection, where a
fine-tuned model for infrared pedestrian detection...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zunhui Xia, Hongxing Li, Libin Lan<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02488v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Medical image recognition serves as a key way to aid in clinical diagnosis,
enabling more accurate and timely identification of diseases and abnormalities.
Vision transformer-based approaches have proven effective in handling various
medical recognition tasks. However, these methods encounter two primary
challenges. First, they are often task-specific and architecture-tailored,
limiting their general applicability. Second, they usually either adopt full
attention to model long-range dependencies...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Weiwei Duan, Luping Ji, Shengjia Chen and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02454v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Different from general object detection, moving infrared small target
detection faces huge challenges due to tiny target size and weak background
contrast.Currently, most existing methods are fully-supervised, heavily relying
on a large number of manual target-wise annotations. However, manually
annotating video sequences is often expensive and time-consuming, especially
for low-quality infrared frame images. Inspired by general object detection,
non-fully supervised strategies ($e.g.$, weakly s...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ayantika Das, Moitreya Chaudhuri, Koushik Bhat and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02405v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Denoising diffusion models produce high-fidelity image samples by capturing
the image distribution in a progressive manner while initializing with a simple
distribution and compounding the distribution complexity. Although these models
have unlocked new applicabilities, the sampling mechanism of diffusion does not
offer means to extract image-specific semantic representation, which is
inherently provided by auto-encoders. The encoding component of auto-encoders
enables mapping between a specific...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Peilin Zhang, Shaouxan Wua, Jun Feng and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02399v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Background and objective: Medical image segmentation is a core task in
various clinical applications. However, acquiring large-scale, fully annotated
medical image datasets is both time-consuming and costly. Scribble annotations,
as a form of sparse labeling, provide an efficient and cost-effective
alternative for medical image segmentation. However, the sparsity of scribble
annotations limits the feature learning of the target region and lacks
sufficient boundary supervision, which poses signif...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Byung Hyun Lee, Wongi Jeong, Woojae Han and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02395v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multiple instance learning (MIL) significantly reduced annotation costs via
bag-level weak labels for large-scale images, such as histopathological whole
slide images (WSIs). However, its adaptability to continual tasks with minimal
forgetting has been rarely explored, especially on instance classification for
localization. Weakly incremental learning for semantic segmentation has been
studied for continual localization, but it focused on natural images,
leveraging global relationships among hun...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Holistic Tokenizer for Autoregressive Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Anlin Zheng, Haochen Wang, Yucheng Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02358v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JaeHyuck Choi, MinJun Kim, JeHyeong Hong<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02314v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LMPNet for Weakly-supervised Keypoint Discovery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Pei Guo, Ryan Farrell<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Haoxuan Li, Chenxu Wei, Haodong Wang and 9 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02307v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to addre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiao Wang, Jingtao Jiang, Qiang Chen and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.CL<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02200v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhuoyang Zhang, Luke J. Huang, Chengyue Wu and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01957v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LongAnimation: Long Animation Generation with Dynamic Global-Local Memory</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Nan Chen, Mengqi Huang, Yihao Meng and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01945v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xin Zhou, Dingkang Liang, Kaijin Chen and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02860v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiaer Xia, Bingkui Tong, Yuhang Zang and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02859v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziqi Miao, Yi Ding, Lijun Li and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02844v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the vi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">No time to train! Training-Free Reference-Based Instance Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Miguel Espinosa, Chenhongyi Yang, Linus Ericsson and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02798v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The performance of image segmentation models has historically been
constrained by the high cost of collecting large-scale annotated data. The
Segment Anything Model (SAM) alleviates this original problem through a
promptable, semantics-agnostic, segmentation paradigm and yet still requires
manual visual-prompts or complex domain-dependent prompt-generation rules to
process a new image. Towards reducing this new burden, our work investigates
the task of object segmentation when provided with, alt...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Liheng Zhang, Lexi Pang, Hang Ye and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02792v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition i...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangfeng Wang, Xiao Li, Yadong Wei and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02790v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangyang Luo, Ye Zhu, Yunfei Liu and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02691v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appea...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziyin Zhou, Yunpeng Luo, Yuanchen Wu and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02664v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Weili Xu, Enxin Song, Wenhao Chai and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02591v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual tok...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Buzhen Huang, Chen Li, Chongyang Xu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02565v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurat...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Luca Parolari, Andrea Cherubini, Lamberto Ballan and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02493v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Automated polyp counting in colonoscopy is a crucial step toward automated
procedure reporting and quality control, aiming to enhance the
cost-effectiveness of colonoscopy screening. Counting polyps in a procedure
involves detecting and tracking polyps, and then clustering tracklets that
belong to the same polyp entity. Existing methods for polyp counting rely on
self-supervised learning and primarily leverage visual appearance, neglecting
temporal relationships in both tracklet feature learning...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hailong Yan, Junjian Huang, Tingwen Huang<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02445v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Current methods for restoring underexposed images typically rely on
supervised learning with paired underexposed and well-illuminated images.
However, collecting such datasets is often impractical in real-world scenarios.
Moreover, these methods can lead to over-enhancement, distorting
well-illuminated regions. To address these issues, we propose IGDNet, a
Zero-Shot enhancement method that operates solely on a single test image,
without requiring guiding priors or training data. IGDNet exhibits ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Holistic Tokenizer for Autoregressive Image Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Anlin Zheng, Haochen Wang, Yucheng Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02358v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rafic Nader, Vincent L'Allinec, Romain Bourcier and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02349v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zecheng Zhao, Selena Song, Tong Chen and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02316v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation
metrics primarily capture visual quality and temporal consistency, offering
limited insight into how synthetic videos perform in downstream tasks such as
text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset
and benchmark designed to evaluate the utility of synthetic videos for building
retrieval models. Based on 800 diverse user queries derived from MSRVTT
training split, we generate synthetic v...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LMPNet for Weakly-supervised Keypoint Discovery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Pei Guo, Ryan Farrell<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hanbo Bi, Yulong Xu, Ya Li and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02294v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on n...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wangbin Ding, Lei Li, Junyi Qiu and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02289v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Myocardial infarction (MI) is a leading cause of death worldwide. Late
gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)
imaging can respectively identify scarring and edema areas, both of which are
essential for MI risk stratification and prognosis assessment. Although
combining complementary information from multi-sequence CMR is useful,
acquiring these sequences can be time-consuming and prohibitive, e.g., due to
the administration of contrast agents. Cine CMR is a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> De Cheng, Zhipeng Xu, Xinyang Jiang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02288v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Juntao Liu, Liqiang Niu, Wenchao Chen and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02279v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens thr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Feizhen Huang, Yu Wu, Yutian Lin and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.MM<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02271v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fanghai Yi, Zehong Zheng, Zexiao Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02270v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) fo...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zeyu Lei, Hongyuan Yu, Jinlin Wu and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02252v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamic...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Understanding Trade offs When Conditioning Synthetic Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Brandon Trabucco, Qasim Wani, Benjamin Pikus and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02217v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulati...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Takuro Kawada, Shunsuke Kitada, Sota Nemoto and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02212v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Active Measurement: Efficient Estimation at Scale</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Max Hamilton, Jinlin Lai, Wenlong Zhao and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01372v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yupeng Zheng, Pengxuan Yang, Zebin Xing and 8 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00603v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world mode...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Epona: Autoregressive Diffusion World Model for Autonomous Driving</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu and 9 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.24113v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Antoine Guédon, Diego Gomez, Nissim Maruani and 3 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.24096v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Shubhabrata Mukherjee, Jack Lang, Obeen Kwon and 4 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV, cs.HC<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.24039v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ruiyang Hao, Bowen Jing, Haibao Yu and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV, cs.RO, I.4.9<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23982v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2E...</div>
                        </div>
                        </div><div class="field-section"><h2 class="field-title">3DGS (98 papers)</h2>
                        <div class="paper">
                            <div class="paper-title">Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuqi Wu, Wenzhao Zheng, Jie Zhou and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02863v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Dense 3D scene reconstruction from an ordered sequence or unordered image
collections is a critical step when bringing research in computer vision into
practical scenarios. Following the paradigm introduced by DUSt3R, which unifies
an image pair densely into a shared coordinate system, subsequent methods
maintain an implicit memory to achieve dense 3D reconstruction from more
images. However, such implicit memory is limited in capacity and may suffer
from information loss of earlier frames. We p...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Qi Xu, Dongxu Wei, Lingzhe Zhao and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02705v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruct...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tan Pan, Zhaorui Tan, Kaiyu Guo and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02581v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Buzhen Huang, Chen Li, Chongyang Xu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02565v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurat...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ruicheng Wang, Sicheng Xu, Yue Dong and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02546v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover th...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhurong Chen, Jinhua Chen, Wei Zhuo and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Echocardiography (echo) plays an indispensable role in the clinical practice
of heart diseases. However, ultrasound imaging typically provides only
two-dimensional (2D) cross-sectional images from a few specific views, making
it challenging to interpret and inaccurate for estimation of clinical
parameters like the volume of left ventricle (LV). 3D ultrasound imaging
provides an alternative for 3D quantification, but is still limited by the low
spatial and temporal resolution and the highly deman...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Seokyeong Lee, Sithu Aung, Junyong Choi and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02393v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Monocular 3D object detection (M3OD) has long faced challenges due to data
scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.
Although various weakly supervised methods and pseudo-labeling methods have
been proposed to address these issues, they are mostly limited by
domain-specific learning or rely solely on shape information from a single
observation. In this paper, we propose a novel pseudo-labeling framework that
uses only video data and is more robust to occlusion, wi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JaeHyuck Choi, MinJun Kim, JeHyeong Hong<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02314v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yunhan Yang, Shuo Chen, Yukun Huang and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02299v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fanghai Yi, Zehong Zheng, Zexiao Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02270v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) fo...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiangxia Chen, Tongyuan Huang, Ke Song<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02250v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state s...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Understanding Trade offs When Conditioning Synthetic Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Brandon Trabucco, Qasim Wani, Benjamin Pikus and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02217v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulati...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rahul Ramachandran, Ali Garjani, Roman Bachmann and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01955v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using establi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ranjan Sapkota, Zhichao Meng, Martin Churuvija and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01912v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the en...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Modality-agnostic, patient-specific digital twins modeling temporally varying digestive motion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jorge Tapias Gomez, Nishant Nadkarni, Lando S. Bosma and 7 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01909v2" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada and 13 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> eess.IV, cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01881v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Shengli Zhou, Jianuo Zhu, Qilin Huang and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.MM<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01800v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zihong Guo, Chen Wan, Yayin Zheng and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01791v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale exampl...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Benjamin Jin, Grant Mair, Joanna M. Wardlaw and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01744v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Lin Wu, Zhixiang Chen, Jianglin Lan<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01737v2" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-bas...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Muzammil Behzad<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01673v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompt...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Camille Billouard, Dawa Derksen, Alexandre Constantin and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01631v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images an...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuxiao Wang, Yu Lei, Zhenao Wei and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01630v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yue-Jiang Dong, Wang Zhao, Jiale Xu and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01603v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhenya Yang, Bingchen Gong, Kai Chen and 1 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00554v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dy...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xingjun Wang, Lianlei Shan<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with lim...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziao Liu, Zhenjia Li, Yifeng Shi and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23611v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Huang, Long Bai, Beilei Cui and 7 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhen Tan, Xieyuanli Chen, Lei Feng and 4 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23207v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hung Nguyen, An Le, Runfa Li and 1 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23042v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Buzhen Huang, Chen Li, Chongyang Xu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02565v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurat...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JaeHyuck Choi, MinJun Kim, JeHyeong Hong<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02314v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zihong Guo, Chen Wan, Yayin Zheng and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01791v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale exampl...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Masks make discriminative models great again!</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00916v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00886v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich lin...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SafeMap: Robust HD Map Construction from Incomplete Observations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiaoshuai Hao, Lingdong Kong, Rong Yin and 4 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziji Lu<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00789v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-base...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhenya Yang, Bingchen Gong, Kai Chen and 1 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00554v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dy...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yingping Liang, Yutao Hu, Wenqi Shao and 1 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00392v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xingjun Wang, Lianlei Shan<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with lim...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Antoine Guédon, Diego Gomez, Nissim Maruani and 3 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.24096v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zinuo You, Stamatios Georgoulis, Anpei Chen and 2 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23957v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and r...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziao Liu, Zhenjia Li, Yifeng Shi and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23611v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GViT: Representing Images as Gaussians for Visual Recognition</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jefferson Hernandez, Ruozhen He, Guha Balakrishnan and 2 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23532v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable re...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhaojie Zeng, Yuesong Wang, Chao Yang and 2 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23479v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Huang, Long Bai, Beilei Cui and 6 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23309v2" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In contemporary surgical research and practice, accurately comprehending 3D
surgical scenes with text-promptable capabilities is particularly crucial for
surgical planning and real-time intra-operative guidance, where precisely
identifying and interacting with surgical tools and anatomical structures is
paramount. However, existing works focus on surgical vision-language model
(VLM), 3D reconstruction, and segmentation separately, lacking support for
real-time text-promptable 3D queries. In this...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Huang, Long Bai, Beilei Cui and 7 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23308v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Autoregressive Denoising Score Matching is a Good Video Anomaly Detector</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hanwen Zhang, Congqi Cao, Qinyi Lv and 2 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23282v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, moti...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Token Activation Map to Visually Explain Multimodal LLMs</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yi Li, Hualiang Wang, Xinpeng Ding and 2 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23270v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant acti...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhen Tan, Xieyuanli Chen, Lei Feng and 4 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23207v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hanyu Zhou, Haonan Wang, Haoyue Liu and 3 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23157v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between backg...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hung Nguyen, An Le, Runfa Li and 1 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23042v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> AmirHossein Naghi Razlighi, Elaheh Badali Golezani, Shohreh Kasaei<br>
                                <strong>Published:</strong> 2025-06-28<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.22973v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaus...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Attention to Burstiness: Low-Rank Bilinear Prompt Tuning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yuzhu Wang, Manni Duan, Shu Kong<br>
                                <strong>Published:</strong> 2025-06-28<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.22908v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian dis...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Robert Beinert, Jonas Bresch<br>
                                <strong>Published:</strong> 2025-06-28<br>
                                <strong>Categories:</strong> math.OC, cs.CV, cs.NA, math.NA, 94A08, 94A12, 65J22, 90C22, 90C25<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.22826v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The handling of manifold-valued data, for instance, plays a central role in
color restoration tasks relying on circle- or sphere-valued color models, in
the study of rotational or directional information related to the special
orthogonal group, and in Gaussian image processing, where the pixel statistics
are interpreted as values on the hyperbolic sheet. Especially, to denoise these
kind of data, there have been proposed several generalizations of total
variation (TV) and Tikhonov-type denoising...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alex Colagrande, Paul Caillon, Eva Feillet and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02748v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the fi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.DC, cs.LG, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02443v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhurong Chen, Jinhua Chen, Wei Zhuo and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Echocardiography (echo) plays an indispensable role in the clinical practice
of heart diseases. However, ultrasound imaging typically provides only
two-dimensional (2D) cross-sectional images from a few specific views, making
it challenging to interpret and inaccurate for estimation of clinical
parameters like the volume of left ventricle (LV). 3D ultrasound imaging
provides an alternative for 3D quantification, but is still limited by the low
spatial and temporal resolution and the highly deman...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rafic Nader, Vincent L'Allinec, Romain Bourcier and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02349v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Farida Siddiqi Prity, Mirza Raquib, Saydul Akbar Murad and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02322v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis De...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wangbin Ding, Lei Li, Junyi Qiu and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02289v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Myocardial infarction (MI) is a leading cause of death worldwide. Late
gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)
imaging can respectively identify scarring and edema areas, both of which are
essential for MI risk stratification and prognosis assessment. Although
combining complementary information from multi-sequence CMR is useful,
acquiring these sequences can be time-consuming and prohibitive, e.g., due to
the administration of contrast agents. Cine CMR is a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">evMLP: An Efficient Event-Driven MLP Architecture for Vision</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhentan Zheng<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01927v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hailong Yan, Ao Li, Xiangtao Zhang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01838v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zihong Guo, Chen Wan, Yayin Zheng and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01791v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale exampl...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Vivek Tetarwal, Sandeep Kumar<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.IT, cs.CV, math.IT<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01778v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them int...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Nora Gourmelon, Marcel Dreier, Martin Mayr and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01747v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the dom...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Camille Billouard, Dawa Derksen, Alexandre Constantin and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01631v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images an...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Controllable Real Image Denoising with Camera Parameters</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Youngjin Oh, Junhyeong Kwon, Keuntek Lee and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01587v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these se...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Lapo Frati, Neil Traft, Jeff Clune and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01559v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-sho...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Crop Pest Classification Using Deep Learning Techniques: A Review</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-bas...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yifei Sun, Marshall A. Dalton, Robert D. Sanders and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> q-bio.NC, cs.AI, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimens...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Learning an Ensemble Token from Task-driven Priors in Facial Analysis</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Sunyong Seo, Semin Kim, Jongha Lee<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01290v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the trainin...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Classification based deep learning models for lung cancer and disease using medical images</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ahmad Chaddad, Jihao Peng, Yihang Wu<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01279v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Rapid Salient Object Detection with Difference Convolutional Neural Networks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhuo Su, Li Liu, Matthias Müller and 4 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01182v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of imag...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Surgical Neural Radiance Fields from One Image</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alberto Neri, Maximilan Fehrentz, Veronica Penza and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00969v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgica...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> David Hunt, Shaocheng Luo, Spencer Hallyburton and 4 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.RO, cs.AR, cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00937v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Masks make discriminative models great again!</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianshi Cao, Marie-Julie Rakotosaona, Ben Poole and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00916v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Anna-Maria Halacheva, Jan-Nico Zaech, Xi Wang and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00886v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich lin...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jisoo Kim, Chu-Hsuan Lin, Alberto Ceballos-Arroyo and 6 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> eess.IV, cs.AI, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00832v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Introduction: Deep learning (DL) models can help detect intracranial
aneurysms on CTA, but high false positive (FP) rates remain a barrier to
clinical translation, despite improvement in model architectures and strategies
like detection threshold tuning. We employed an automated, anatomy-based,
heuristic-learning hybrid artery-vein segmentation post-processing method to
further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D
convolutional neural network-transformer hybrid (3D-CNN...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yasser El Jarida, Youssef Iraqi, Loubna Mekouar<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00822v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present ...</div>
                        </div>
                        </div><div class="field-section"><h2 class="field-title">NeRF (95 papers)</h2>
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alex Colagrande, Paul Caillon, Eva Feillet and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02748v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the fi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Teng Fu, Yuwen Chen, Zhuofan Chen and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02479v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multi-object tracking is a classic field in computer vision. Among them,
pedestrian tracking has extremely high application value and has become the
most popular research category. Existing methods mainly use motion or
appearance information for tracking, which is often difficult in complex
scenarios. For the motion information, mutual occlusions between objects often
prevent updating of the motion state; for the appearance information,
non-robust results are often obtained due to reasons such a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.DC, cs.LG, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02443v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhurong Chen, Jinhua Chen, Wei Zhuo and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Echocardiography (echo) plays an indispensable role in the clinical practice
of heart diseases. However, ultrasound imaging typically provides only
two-dimensional (2D) cross-sectional images from a few specific views, making
it challenging to interpret and inaccurate for estimation of clinical
parameters like the volume of left ventricle (LV). 3D ultrasound imaging
provides an alternative for 3D quantification, but is still limited by the low
spatial and temporal resolution and the highly deman...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">UVLM: Benchmarking Video Language Model for Underwater World Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xizhe Xue, Yang Zhou, Dawei Yan and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02373v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recently, the remarkable success of large language models (LLMs) has achieved
a profound impact on the field of artificial intelligence. Numerous advanced
works based on LLMs have been proposed and applied in various scenarios. Among
them, video language models (VidLMs) are particularly widely used. However,
existing works primarily focus on terrestrial scenarios, overlooking the highly
demanding application needs of underwater observation. To overcome this gap, we
introduce UVLM, an under water...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rafic Nader, Vincent L'Allinec, Romain Bourcier and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02349v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Farida Siddiqi Prity, Mirza Raquib, Saydul Akbar Murad and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02322v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis De...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wangbin Ding, Lei Li, Junyi Qiu and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02289v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Myocardial infarction (MI) is a leading cause of death worldwide. Late
gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)
imaging can respectively identify scarring and edema areas, both of which are
essential for MI risk stratification and prognosis assessment. Although
combining complementary information from multi-sequence CMR is useful,
acquiring these sequences can be time-consuming and prohibitive, e.g., due to
the administration of contrast agents. Cine CMR is a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">evMLP: An Efficient Event-Driven MLP Architecture for Vision</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhentan Zheng<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01927v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ranjan Sapkota, Zhichao Meng, Martin Churuvija and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01912v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the en...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hailong Yan, Ao Li, Xiangtao Zhang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01838v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zihong Guo, Chen Wan, Yayin Zheng and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01791v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale exampl...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Vivek Tetarwal, Sandeep Kumar<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.IT, cs.CV, math.IT<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01778v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them int...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Nora Gourmelon, Marcel Dreier, Martin Mayr and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01747v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the dom...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Camille Billouard, Dawa Derksen, Alexandre Constantin and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01631v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images an...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Controllable Real Image Denoising with Camera Parameters</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Youngjin Oh, Junhyeong Kwon, Keuntek Lee and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01587v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these se...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Lapo Frati, Neil Traft, Jeff Clune and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01559v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-sho...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Interpolation-Based Event Visual Data Filtering Algorithms</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Marcin Kowlaczyk, Tomasz Kryjak<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01557v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tapas K. Dutta, Snehashis Majhi, Deepak Ranjan Nayak and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01509v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps w...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Crop Pest Classification Using Deep Learning Techniques: A Review</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-bas...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jonáš Herec, Vít Růžička, Rado Pitoňák<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.LG, cs.PF<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01472v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detec...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wenjie Liu, Bingshu Wang, Ze Wang and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01422v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow so...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yifei Sun, Marshall A. Dalton, Robert D. Sanders and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> q-bio.NC, cs.AI, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimens...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Physics-informed Ground Reaction Dynamics from Human Motion Capture</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Cuong Le, Huy-Phuong Le, Duc Le and 3 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01340v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a signific...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Camille Billouard, Dawa Derksen, Alexandre Constantin and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01631v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images an...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Surgical Neural Radiance Fields from One Image</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alberto Neri, Maximilan Fehrentz, Veronica Penza and 2 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00969v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgica...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xin Yang, Ruiming Du, Hanyang Huang and 7 others<br>
                                <strong>Published:</strong> 2025-07-01<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.00371v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziao Liu, Zhenjia Li, Yifeng Shi and 1 others<br>
                                <strong>Published:</strong> 2025-06-30<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23611v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Dynamic View Synthesis from Small Camera Motion Videos</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Huiqiang Sun, Xingyi Li, Juewen Peng and 4 others<br>
                                <strong>Published:</strong> 2025-06-29<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2506.23153v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parame...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Alex Colagrande, Paul Caillon, Eva Feillet and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02748v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the fi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.DC, cs.LG, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02443v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs,...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhurong Chen, Jinhua Chen, Wei Zhuo and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Echocardiography (echo) plays an indispensable role in the clinical practice
of heart diseases. However, ultrasound imaging typically provides only
two-dimensional (2D) cross-sectional images from a few specific views, making
it challenging to interpret and inaccurate for estimation of clinical
parameters like the volume of left ventricle (LV). 3D ultrasound imaging
provides an alternative for 3D quantification, but is still limited by the low
spatial and temporal resolution and the highly deman...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiahao Wu, Rui Peng, Jianbo Jiao and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02363v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rafic Nader, Vincent L'Allinec, Romain Bourcier and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02349v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Farida Siddiqi Prity, Mirza Raquib, Saydul Akbar Murad and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02322v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis De...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Perception Activator: An intuitive and portable framework for brain cognitive exploration</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Le Xu, Qi Zhang, Qixian Zhang and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02311v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstru...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yunhan Yang, Shuo Chen, Yukun Huang and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02299v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wangbin Ding, Lei Li, Junyi Qiu and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02289v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Myocardial infarction (MI) is a leading cause of death worldwide. Late
gadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)
imaging can respectively identify scarring and edema areas, both of which are
essential for MI risk stratification and prognosis assessment. Although
combining complementary information from multi-sequence CMR is useful,
acquiring these sequences can be time-consuming and prohibitive, e.g., due to
the administration of contrast agents. Cine CMR is a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Understanding Trade offs When Conditioning Synthetic Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Brandon Trabucco, Qasim Wani, Benjamin Pikus and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02217v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulati...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">evMLP: An Efficient Event-Driven MLP Architecture for Vision</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhentan Zheng<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01927v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hailong Yan, Ao Li, Xiangtao Zhang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01838v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incre...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zihong Guo, Chen Wan, Yayin Zheng and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01791v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale exampl...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Vivek Tetarwal, Sandeep Kumar<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.IT, cs.CV, math.IT<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01778v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them int...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Nora Gourmelon, Marcel Dreier, Martin Mayr and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01747v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the dom...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Camille Billouard, Dawa Derksen, Alexandre Constantin and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01631v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images an...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Towards Controllable Real Image Denoising with Camera Parameters</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Youngjin Oh, Junhyeong Kwon, Keuntek Lee and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01587v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these se...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Lapo Frati, Neil Traft, Jeff Clune and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01559v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-sho...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Crop Pest Classification Using Deep Learning Techniques: A Review</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-bas...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Chentao Shen, Ding Pan, Mingyu Mei and 2 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01478v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rende...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yifei Sun, Marshall A. Dalton, Robert D. Sanders and 5 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> q-bio.NC, cs.AI, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimens...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Shuai Tan, Bill Gong, Bin Ji and 1 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01390v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these f...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tianrui Lou, Xiaojun Jia, Siyuan Liang and 4 others<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01367v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to ob...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Learning Camera-Agnostic White-Balance Preferences</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Luxi Zhao, Mahmoud Afifi, Michael S. Brown<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01342v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Learning an Ensemble Token from Task-driven Priors in Facial Analysis</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Sunyong Seo, Semin Kim, Jongha Lee<br>
                                <strong>Published:</strong> 2025-07-02<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.01290v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the trainin...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Renhao Wang, Haoran Geng, Tingle Li and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.RO, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02864v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Robots must integrate multiple sensory modalities to act effectively in the
real world. Yet, learning such multimodal policies at scale remains
challenging. Simulation offers a viable solution, but while vision has
benefited from high-fidelity simulators, other modalities (e.g. sound) can be
notoriously difficult to simulate. As a result, sim-to-real transfer has
succeeded primarily in vision-based tasks, with multimodal transfer still
largely unrealized. In this work, we tackle these challenges...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhening Huang, Xiaoyang Wu, Fangcheng Zhong and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02861v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the resul...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">RefTok: Reference-Based Tokenization for Video Generation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiang Fan, Xiaohang Sun, Kushan Thakkar and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02862v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Effectively handling temporal redundancy remains a key challenge in learning
video models. Prevailing approaches often treat each set of frames
independently, failing to effectively capture the temporal dependencies and
redundancies inherent in videos. To address this limitation, we introduce
RefTok, a novel reference-based tokenization method capable of capturing
complex temporal dynamics and contextual information. Our method encodes and
decodes sets of frames conditioned on an unquantized ref...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xin Zhou, Dingkang Liang, Kaijin Chen and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02860v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AnyI2V: Animating Any Conditional Image with Motion Control</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziye Li, Hao Luo, Xincheng Shuai and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02857v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recent advancements in video generation, particularly in diffusion models,
have driven notable progress in text-to-video (T2V) and image-to-video (I2V)
synthesis. However, challenges remain in effectively integrating dynamic motion
signals and flexible spatial constraints. Existing T2V methods typically rely
on text prompts, which inherently lack precise control over the spatial layout
of generated content. In contrast, I2V methods are limited by their dependence
on real images, which restricts ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Ziqi Miao, Yi Ding, Lijun Li and 1 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02844v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the vi...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Panpan Ji, Junni Song, Hang Xiao and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02826v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extrac...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Fangfu Liu, Hao Li, Jiawei Chi and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02813v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangSc...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Gent Serifi, Marcel C. Bühler<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.GR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02803v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian pri...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangfeng Wang, Xiao Li, Yadong Wei and 8 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CL<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02790v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) ...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Danrong Zhang, Huili Huang, N. Simrill Smith and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.SI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02781v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage sever...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">DexVLG: Dexterous Vision-Language-Grasp Model at Scale</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Jiawei He, Danshi Li, Xinqiang Yu and 7 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.RO<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02747v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">As large models gain traction, vision-language-action (VLA) systems are
enabling robots to tackle increasingly complex tasks. However, limited by the
difficulty of data collection, progress has mainly focused on controlling
simple gripper end-effectors. There is little research on functional grasping
with large models for human-like dexterous hands. In this paper, we introduce
DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction
aligned with language instructions using...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Prompt learning with bounding box constraints for medical image segmentation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Mélanie Gaillochet, Mehrdad Noori, Sahar Dastani and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02743v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Pixel-wise annotations are notoriously labourious and costly to obtain in the
medical domain. To mitigate this burden, weakly supervised approaches based on
bounding box annotations-much easier to acquire-offer a practical alternative.
Vision foundation models have recently shown noteworthy segmentation
performance when provided with prompts such as points or bounding boxes. Prompt
learning exploits these models by adapting them to downstream tasks and
automating segmentation, thereby reducing u...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Xiangyang Luo, Ye Zhu, Yunfei Liu and 5 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02691v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appea...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">APT: Adaptive Personalized Training for Diffusion Models with Limited Data</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> JungWoo Chae, Jiyoon Kim, JaeWoong Choi and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI, 60J60, 68T07, I.2.6; I.2.10; I.4.9<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02687v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's in...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Learning few-step posterior samplers by unfolding and distillation of diffusion models</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Charlesquin Kemajou Mbakam, Jonathan Spence, Marcelo Pereyra<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02686v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Diffusion models (DMs) have emerged as powerful image priors in Bayesian
computational imaging. Two primary strategies have been proposed for leveraging
DMs in this context: Plug-and-Play methods, which are zero-shot and highly
flexible but rely on approximations; and specialized conditional DMs, which
achieve higher accuracy and faster inference for specific tasks through
supervised training. In this work, we introduce a novel framework that
integrates deep unfolding and model distillation to t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Real-time Image-based Lighting of Glints</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tom Kneiphof, Reinhard Klein<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.GR, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02674v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is gr...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Fair Deepfake Detectors Can Generalize</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Harry Cheng, Ming-Hui Liu, Yangyang Guo and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02645v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distrib...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hazal Mogultay Ozcan, Sinan Kalkan, Fatos T. Yarman-Vural<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.LG, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02619v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Tan Pan, Zhaorui Tan, Kaiyu Guo and 6 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02581v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Abiam Remache González, Meriem Chagour, Timon Bijan Rüth and 12 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, I.2.10; I.4.8<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02519v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">This paper introduces IMASHRIMP, an adapted system for the automated
morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing
genetic selection tasks in aquaculture. Existing deep learning and computer
vision techniques were modified to address the specific challenges of shrimp
morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination
modules, based on a modified ResNet-50 architecture, to classify images by the
point of view and determine rostrum inte...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Hyunsoo Son, Jeonghyun Noh, Suemin Jeon and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02494v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address t...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zunhui Xia, Hongxing Li, Libin Lan<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02488v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Medical image recognition serves as a key way to aid in clinical diagnosis,
enabling more accurate and timely identification of diseases and abnormalities.
Vision transformer-based approaches have proven effective in handling various
medical recognition tasks. However, these methods encounter two primary
challenges. First, they are often task-specific and architecture-tailored,
limiting their general applicability. Second, they usually either adopt full
attention to model long-range dependencies...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Teng Fu, Yuwen Chen, Zhuofan Chen and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.AI<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02479v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multi-object tracking is a classic field in computer vision. Among them,
pedestrian tracking has extremely high application value and has become the
most popular research category. Existing methods mainly use motion or
appearance information for tracking, which is often difficult in complex
scenarios. For the motion information, mutual occlusions between objects often
prevent updating of the motion state; for the appearance information,
non-robust results are often obtained due to reasons such a...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Wei Li, Jingyang Zhang, Lihao Liu and 4 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02437v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a
source model to unseen medical sites using unlabeled test data, due to the high
cost of data annotation. Existing TTA methods consider scenarios where data
from one or multiple domains arrives in complete domain units. However, in
clinical practice, data usually arrives in domain fragments of arbitrary
lengths and in random arrival orders, due to resource constraints and patient
variability. This paper investigates a p...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Yiming Zhong, Xiaolin Zhang, Ligang Liu and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02419v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabli...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Determination Of Structural Cracks Using Deep Learning Frameworks</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Subhasis Dasgupta, Jaydip Sen, Tuhina Halder<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.LG, eess.IV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02416v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Structural crack detection is a critical task for public safety as it helps
in preventing potential structural failures that could endanger lives. Manual
detection by inexperienced personnel can be slow, inconsistent, and prone to
human error, which may compromise the reliability of assessments. The current
study addresses these challenges by introducing a novel deep-learning
architecture designed to enhance the accuracy and efficiency of structural
crack detection. In this research, various con...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">Privacy-preserving Preselection for Face Identification Based on Packing</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Rundong Xin, Taotao Wang, Jin Wang and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV, cs.CR<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02414v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Face identification systems operating in the ciphertext domain have garnered
significant attention due to increasing privacy concerns and the potential
recovery of original facial data. However, as the size of ciphertext template
libraries grows, the face retrieval process becomes progressively more
time-intensive. To address this challenge, we propose a novel and efficient
scheme for face retrieval in the ciphertext domain, termed Privacy-Preserving
Preselection for Face Identification Based on...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Zhurong Chen, Jinhua Chen, Wei Zhuo and 2 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> eess.IV, cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02411v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Echocardiography (echo) plays an indispensable role in the clinical practice
of heart diseases. However, ultrasound imaging typically provides only
two-dimensional (2D) cross-sectional images from a few specific views, making
it challenging to interpret and inaccurate for estimation of clinical
parameters like the volume of left ventricle (LV). 3D ultrasound imaging
provides an alternative for 3D quantification, but is still limited by the low
spatial and temporal resolution and the highly deman...</div>
                        </div>
                        
                        <div class="paper">
                            <div class="paper-title">A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern</div>
                            <div class="paper-meta">
                                <strong>Authors:</strong> Duong Nguyen-Ngoc Tran, Long Hoang Pham, Chi Dai Tran and 3 others<br>
                                <strong>Published:</strong> 2025-07-03<br>
                                <strong>Categories:</strong> cs.CV<br>
                                <strong>URL:</strong> <a href="http://arxiv.org/pdf/2507.02408v1" target="_blank">View Paper</a>
                            </div>
                            <div class="paper-summary">Multi-Object Tracking in thermal images is essential for surveillance
systems, particularly in challenging environments where RGB cameras struggle
due to low visibility or poor lighting conditions. Thermal sensors enhance
recognition tasks by capturing infrared signatures, but a major challenge is
their low-level feature representation, which makes it difficult to accurately
detect and track pedestrians. To address this, the paper introduces a novel
tuning method for pedestrian tracking, specifi...</div>
                        </div>
                        </div></body></html>